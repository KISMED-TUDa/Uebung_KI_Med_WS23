{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mudi5c5wzPY-",
        "UCJ-CZvgd8Wq",
        "cQuNFUdpzZ8O"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoYGq1dirp-y"
      },
      "source": [
        "# Dimensionsreduktion\n",
        "\n",
        "**Hinweis 1:** In der Vorlesung wird die Datenmatrix $\\boldsymbol X$ so eingeführt, dass jede Zeile ein Feature repräsentiert und jede Spalte einen Datenpunkt. Mit der Singulärwertzerlegung $\\boldsymbol X = \\boldsymbol U \\boldsymbol S \\boldsymbol V^H$ führt dies zur Formel $\\boldsymbol Y = \\boldsymbol\\Gamma^T\\boldsymbol X$. In Aufgabe 1 wird dieselbe Repräsentation der Daten gewählt. \n",
        "\n",
        "Die Repräsentation von digitalen Daten ist allerdings häufig anders herum, jede Spalte steht dann für ein Feature und jede Zeile für einen Datenpunkt. Dies ist auch in Aufgabe 2 und 3 bei der Verwendung echter Daten der Fall. Daher ist es für diese Aufgaben notwendig, die Datenmatrix zu transponieren.\n",
        "\n",
        "**Hinweis 2:** In der Vorlesung wird die PCA am Beispiel eines EKG-Signals eingeführt. Im Gegensatz dazu verwenden wir in Aufgabe 2 einen Datensatz, der aus verschiedensten Features besteht. Diese Features unterscheiden sich alle in ihrer Skalierung. Man muss daher die Features nicht nur mittelwertfrei machen, sondern auch noch auf eine Standardabweichung von 1 bringen. Ansonsten würde die Einheit der Features eine Rolle spielen; zum Beispiel hätte eine Strecke, die in mm angegeben wird dann einen sehr viel Größeren Einfluss als eine gleichlange Strecke, die in km angegeben wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mudi5c5wzPY-"
      },
      "source": [
        "## 1. Matrizenrechnung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgkJ1pJ_eHMw"
      },
      "source": [
        "Gegeben sei die Matrix \n",
        "$\\boldsymbol X=\\begin{bmatrix} 1 & -1 & 0\\\\ 2 & 1 & -3\\end{bmatrix} \\in\\mathbb{R}^{2\\times 3}$. \n",
        "\n",
        "**a)** Berechne per Hand die Eigenwerte und Eigenvektoren der Matrizen $\\boldsymbol{X}\\boldsymbol X^H$ und $\\boldsymbol{X}^H\\boldsymbol X$ und gib die Eigenwertzerlegungen an."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMhADArtQQ2h"
      },
      "source": [
        "**b)** Berechne jetzt die Kovarianz von $\\boldsymbol X$: $Cov[\\boldsymbol X] = \\boldsymbol\\Sigma = E\\Big[(\\boldsymbol X-E[\\boldsymbol X]) (\\boldsymbol X-E[\\boldsymbol X])^H\\Big]$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZQneE5gvE07"
      },
      "source": [
        "**c)** Bestimme die Eigenwertzerlegung der in b) berechneten Kovarianz und gib die Ladungsmatrix $\\boldsymbol\\Gamma$ der Matrix $\\boldsymbol X$ an. Überprüfe Dein Ergebnis, indem Du mit Python die Singulärwertzerlegung von $\\boldsymbol X$ berechnen. Was stellst du fest?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCJ-CZvgd8Wq"
      },
      "source": [
        "##2. Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ3jT8lfXKJg"
      },
      "source": [
        "Die Hauptkomponentenanalyse oder Principal Component Analysis (PCA) ist ein Verfahren zur Strukturierung von großen Datensätzen mit vielen Features. Diese Strukturierung kann in erster Linie für zwei Anwendungen genutzt werden:\n",
        "\n",
        "*   Vereinfachung der Klassifikation von Daten \n",
        "*   Veranschaulichung höherdimensionaler Matrizen\n",
        "\n",
        "Im Folgenden werden wir eine PCA auf einem Datensatz zu chronischen Nierenerkrankungen durchführen (chronic kidney diseases, *ckd*). Die 26 Spalten enthalten 24 Features, eine ID und eine Klassifikation. Jede Zeile steht für eine untersuchte Person. Die Quelle des Datensatzes und weitere Informationen zu den Features findest Du [hier](https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsUBJo2WHKyy"
      },
      "source": [
        "Führe die folgenden zwei Zellen aus, um den Datensatz `kidney_disease.csv` zu importieren und zu bereinigen. Die Bereinigung beinhaltet die Löschung aller Zeilen mit mindestens einem `NaN`-Wert, sowie der Spalte `id`, die keinen Informationsgehalt hat. Außerdem werden die kategorischen Features (z.B. das Feature `appet`, das den Appetit der Person als *good* oder *poor* beschreibt), in numerische Werte transformiert. Zuletzt wird das Dataframe `kidney_dataset` noch in die Featurematrix `X` und den Zielvektor `true_label` aufgeteilt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO_scrs4DEJ7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "kidney_dataset = pd.read_csv('/content/gdrive/MyDrive/kidney_disease.csv')\n",
        "kidney_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ccsK0VDSvr"
      },
      "source": [
        "# Bereinigung des Datensatzes von NaN-Werten und der Spalte id\n",
        "kidney_dataset = kidney_dataset.dropna()\n",
        "kidney_dataset = kidney_dataset.drop(columns='id')\n",
        "\n",
        "# Umwandlung der kategorischen Features in Boolean mithilfe von Dictionaries und der replace-Funktion von Pandas\n",
        "# rbc, pc: abnormal = 1, normal = 0\n",
        "trans1={'abnormal': True, 'normal': False}\n",
        "# pcc, ba: present = 1, notpresent = 0\n",
        "trans2={'present': True, 'notpresent': False}\n",
        "# htn, dm, cad, pe, ane: yes =1, no = 0\n",
        "trans3={'yes': True, 'no': False}\n",
        "# appet: good=1, poor = 0\n",
        "trans4={'good': True, 'poor': False}\n",
        "kidney_dataset[['rbc','pc']]=kidney_dataset[['rbc','pc']].replace(trans1)\n",
        "kidney_dataset[['pcc','ba']]=kidney_dataset[['pcc','ba']].replace(trans2)\n",
        "kidney_dataset[['htn','dm','cad','pe','ane']]=kidney_dataset[['htn','dm','cad','pe','ane']].replace(trans3)\n",
        "kidney_dataset['appet']=kidney_dataset['appet'].replace(trans4)\n",
        "\n",
        "# Erstellen eines eigenen Dataframes für die wahren labels\n",
        "true_label = kidney_dataset['classification']\n",
        "# Löschen der wahren labels aus dem Featureframe -> die wahren labels sollen nicht weiter vorverarbeitet werden!\n",
        "X=kidney_dataset.drop(columns='classification')\n",
        "\n",
        "kidney_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gyx3rObvKQ"
      },
      "source": [
        "Zur Berechnung der PCA muss jedes Feature standardisiert werden. Das heißt, dass die Features mittelwertfrei sein und eine Standardabweichung von 1 haben müssen - man nimmt also an, dass sie standardnormalverteilt sind. In unserem Fall heißt das, dass Mittelwert und Standardabweichung für jede **Spalte** der Matrix $\\boldsymbol X$ berechnet und normiert werden müssen, denn jede Spalte steht für ein Feature - anders als in der VL oder Aufgabe 1! Dies Skalierung könnt ihr beispielsweise mit der Bibliothek `sklearn` machen, die dafür über den `StandardScaler` verfügt. Genausogut könnt ihr aber auch einfach Mittelwerte berechnen und subtrahieren, sowie die Standardabweichung berechnen und durch sie teilen. Es ergibt sich die Matrix $\\tilde{\\boldsymbol X}$. \n",
        "\n",
        "**Achtung:** Mehrere der Spalten sind vom Typ `Object`, was keine manuelle Rechnung zulässt (der `StandardScaler` hingegen hat damit keine Probleme). Mit dem Befehl `pd.to_numeric` kann man sie in einen numerischen Datentyp ändern. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQLMnLLoTdQ"
      },
      "source": [
        "**a)** Erstelle das DataFrame `X_tilde`, das die standardisierten Features enthält!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgs6DzB4DeYC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUwyXzz5ewZ4"
      },
      "source": [
        "**b)** Führe dann die PCA mit zwei Komponenten aus. Füge das erhaltene DataFrame mit den *true labels* zusammen und erstelle einen zweidimensionalen Scatterplot der Klassifikation über den Komponenten. Zum Plotten stellen wir euch in der folgenden Zelle die Funktion `draw_pca_scatterplot(pca_dataframe, figax, pal=palette)` zur Verfügung, die ihr verwenden könnt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ6__cI6Fx3_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "palette = sns.color_palette(\"colorblind\")\n",
        "\n",
        "def draw_pca_scatterplot(pca_dataframe, figax, pal=palette):\n",
        "  \"\"\"\n",
        "  Zeichne einen Scatterplot des Dataframes pca_dataframe\n",
        "\n",
        "  :param pca_dataframe: Zu plottendes Dataframe, muss 2 Features mit Namen 'principal component 1' und 'principal component 2' und das Label mit Namen 'classification' enthalten;\n",
        "                        falls classifier=True, muss außerdem eine Spalte 'prediciton' vorhanden sein\n",
        "  :param figax: Die Figure oder Achse, auf der zu plotten ist\n",
        "  :param pal: Die Farbpalette, mit der geplottet werden soll\n",
        "  \"\"\"\n",
        "  targets = ['ckd','notckd']\n",
        "  colors = [pal[0],pal[1]]\n",
        "\n",
        "  figax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  figax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = pca_dataframe['classification'] == target\n",
        "      figax.scatter(pca_dataframe.loc[indicesToKeep, 'principal component 1'], pca_dataframe.loc[indicesToKeep, 'principal component 2'], color = color, s = 50)\n",
        "  figax.legend(targets)\n",
        "  figax.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2VLfHhDn48A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQuNFUdpzZ8O"
      },
      "source": [
        "## 3. k-Nearest-Neigbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2smgaRTDQNf"
      },
      "source": [
        "Zum Abschluss wollen wir die PCA nutzen, um eine einfache kNN-Klassifikation durchzuführen. \n",
        "\n",
        "**a)** Führe die nächsten beiden Zellen aus, um den `KNeighborsClassifier` der Bibliothek `sklearn` zu importieren und das per `sklearn` erstellte Dataframe aufzuteilen in einen Teil der fürs Training verwendet wird und einen fürs Testen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liR6mtYs59Ze"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "import random as rd\n",
        "rd.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkWDvtxM7uG2"
      },
      "source": [
        "train_fraction = 0.8\n",
        "\n",
        "train_index = np.sort(rd.sample(range(0,158),int(train_fraction*len(principal_dataframe))))\n",
        "train_dataframe = principal_dataframe.iloc[train_index]\n",
        "train_truth = true_label.iloc[train_index]\n",
        "combined_trainframe = pd.concat([train_dataframe,train_truth],axis=1)\n",
        "\n",
        "test_mask = np.ones(principal_dataframe.shape[0],dtype=bool)\n",
        "test_mask[train_index] = False\n",
        "test_dataframe = principal_dataframe.iloc[test_mask]\n",
        "test_truth = true_label.iloc[test_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLkNPbpb-L0l"
      },
      "source": [
        "**b)** Trainiere jetzt einen kNN-Klassifizierer mit den Trainingsdaten und teste ihn an den Testdaten. Probiere verschiedene Werte für *k* aus und schaue dir die Ergebnisse an. Verwende den eben importierten Klassifizierer `KNeighborsClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BSqS0nQIitv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing: Statistische Wortmodelle\n",
        "\n",
        "In der 10. Übung machen wir einen kleinen Exkurs in die statistischen Modelle, um genau zu sein die n-Gram Modelle.\n",
        "\n",
        "Das Ziel dieser Übung ist es  ein generatives Modell aufzustellen, das, nachdem es auf einer Liste von Medikamentennamen die statistische Beschaffenheit dieser gelernt hat, zufällig neue, vorher nicht gesehene Namen generieren kann. In diesem Fall stellen die Tokens also keine Wörter, sondern einfache Buchstaben, Ziffern und andere Zeichen dar, was das Vokabular sehr klein hält und rechenintensive Vorgänge vermeidet. *Im Wesentlichen sollen also die Modelle und Grafik von Folie 17 der Vorlesung (Folie 28 im Video zur VL) Schritt für Schritt nachgebaut werden.*"
      ],
      "metadata": {
        "id": "6YTJL-jQysiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir benötigen Pandas um mit tabellarischen Daten zu arbeiten. Die Medikamenten Namen ziehen wir aus einer Auflistung zu verschriebenen Medikamenten bestimmter Ärzte."
      ],
      "metadata": {
        "id": "d7YVfGy21_cQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNyZRcRrIb6c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir benötigen im Folgenden die Datei `'medicine_prescription_records.csv'`. Zuerst laden wir die Daten aus der csv-Datei als Pandas Dataframe und schauen uns die Beschaffenheit der Daten an."
      ],
      "metadata": {
        "id": "yaB6ittC2f7g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJc51dzcIrQ6"
      },
      "source": [
        "# read_csv scheitert manchmal, wenn die große csv-Datei auf einmal gelesen werden soll\n",
        "\n",
        "for retry in range(1,16):\n",
        "  try:\n",
        "    print('Versuche große Datei zu laden',retry)\n",
        "    chunks = pd.read_csv('medicine_prescription_records.csv',header=0, chunksize=1000)\n",
        "    df = pd.concat(chunks)\n",
        "  except:\n",
        "    print('Fehlgeschlagen!')\n",
        "    time.sleep(0.5)\n",
        "    continue\n",
        "  break\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uns interessiert nur die letzte Spalte mit den Medikamenten."
      ],
      "metadata": {
        "id": "9siy9BCx22_-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOYZSspRKYbv"
      },
      "source": [
        "texts = df.iloc[:,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Extrahiere die Namen aller Medikamente, sodass du eine Liste mit einzigartigen Strings erhälst"
      ],
      "metadata": {
        "id": "yz9NCuSq3Dwx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vtfFVAlKoWw"
      },
      "source": [
        "# DEIN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnrcgqNyLFn3",
        "outputId": "a24e3655-3e95-4ed8-833d-34fe93583343"
      },
      "source": [
        "print('Anzahl an Medikamenten',len(words_unique))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl an Medikamenten 2397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Anzahl an Medikamenten: 2397`"
      ],
      "metadata": {
        "id": "WRCfxM1S3yxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Implementiere die Funktion `prepare_n_gram`, welche eine Liste mit Wörtern und ein n (für das n-Gram) erhält und die Wörter tokenized und **entsprechend des Modell-Grades** um Start- und Endtokens ergänzt. Ein Token entspricht hierbei gerade einem Zeichen. Die neue Liste sollte also  beispielsweise Wörter der Form `['<s>','A','S','P','I','R','I','N','</s>'] `enthalten."
      ],
      "metadata": {
        "id": "KLic28sq4NrF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNu70F0IM1sN"
      },
      "source": [
        "def prepare_n_gram(list_words,n):\n",
        "  # DEIN CODE\n",
        "  return sep_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78OPM2DuOuxl"
      },
      "source": [
        "sep_words = prepare_n_gram(words_unique,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Implementiere die Funktion `train_model`, welche tokenized Trainingsdaten und das gewünschte n>1 (also minimal bigram) erhält und ein generatives Modell für das n-Gram zurückgibt, also auf Basis eines übergebenen n-Grams die Wahrscheinlichkeitsverteilung über den nächsten Token zurückgibt. Das Modell soll ein Dictionary der Form {$q$ : $P(T_k=t|T_{k-n+1:k-1}=q)$} sein.\n",
        "Außerdem soll das verwendete Vokabular zurückgegeben werden. Das ist eine Liste, welche zu den Wahrscheinlichkeiten die entspechend geordneten Tokens enthält.\n",
        "\n",
        "*Anmerkung: Da das Vokabular in diesem Fall nur aus einzelnen Buchstaben besteht und wir nur aus diesem Samplen werden, ist es nicht erforderlich extra Tokens für unbekannte Wörter/Symbole anzulegen.*"
      ],
      "metadata": {
        "id": "TY6Ahh5C6J5d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oalaf61TO9KD"
      },
      "source": [
        "def train_model(prepared_data,n):\n",
        "  '''\n",
        "  Params:\n",
        "    prepared_data: List[List[str]]\n",
        "    n: int\n",
        "  Return:\n",
        "    model: Dict[str,ndarray]\n",
        "    symols: List[str]\n",
        "      Liste der Tokens aus dem Vokabular\n",
        "\n",
        "  '''\n",
        "  assert n>1\n",
        "  model = dict() # {n_gram: ndarray}\n",
        "\n",
        "  # DEIN CODE\n",
        "\n",
        "\n",
        "  return model,symbols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urobLdz_LI6m"
      },
      "source": [
        "model, symbols = train_model(sep_words,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Du kannst folgende Funktion verwenden um anhand eines Arrays mit Wahrscheinlichkeiten Indices zu samplen."
      ],
      "metadata": {
        "id": "zm3KlJEtB-4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(a,n=1):\n",
        "  '''\n",
        "  Sampled einen Index (oder mehrere) aus einer Wahrscheinlichkeitsverteilung, welche als Array vorliegt.\n",
        "  Params:\n",
        "    a: ndarray\n",
        "      Muss eine Wahrscheinlichkeitsverteilung darstellen\n",
        "    n: int\n",
        "      Anzahl der Samples\n",
        "  Return\n",
        "    index: int if n==1\n",
        "           ndarray if n>1\n",
        "\n",
        "  '''\n",
        "  choices = np.prod(a.shape)\n",
        "  index = np.random.choice(choices, size=n, p=a.ravel())\n",
        "  if n==1:\n",
        "    return index[0]  # ndarray[n_sample,n_symbol]\n",
        "  else:\n",
        "    return index\n",
        "\n",
        "print(sample(model['OC']))"
      ],
      "metadata": {
        "id": "9qdKTF4-B93x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Implementiere die Funktion `sample_model`, welche aus einem n-Gram Modell ein neues Wort der Form `['<s>','A','S','P','I','R','I','N','</s>']` sampled und terminiert, sobald die entsprechend Anzahl Ende-Tokens erreicht ist."
      ],
      "metadata": {
        "id": "Lz50hb4VCRpd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztFLP3bZbVD"
      },
      "source": [
        "def sample_model(model,symbols,n):\n",
        "\n",
        "  # DEIN CODE\n",
        "\n",
        "  return word\n",
        "\n",
        "def make_nice(word,n):\n",
        "  '''\n",
        "  Übersetzt tokenized Wort in String\n",
        "  '''\n",
        "  if n == 1:\n",
        "    word = \"\".join(word)\n",
        "  else:\n",
        "    word = \"\".join(word[n-1:-n+1])\n",
        "  return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) Teste nun dein Modell, indem du ein *n* wählst, die Trainingsdaten vorbereitest, ein Modell erstellst und aus diesem Modell ein Wort samplest und dieses \"schön\" darstellst.\n",
        "\n",
        "Ist das Wort eine neue Kreation oder bereits in den Trainingsdaten enthalten?"
      ],
      "metadata": {
        "id": "VKvn1xBCDOLt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi6rex-ec0Uf"
      },
      "source": [
        "# DEIN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7PK3vlXeBq_"
      },
      "source": [
        "# DEIN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f) Bislang ist das implementierte Modell kontextsensitiv. Implementiere nun außerdem das generative \"bag of words\" Modell (n=1)"
      ],
      "metadata": {
        "id": "7YEHiwdcDzhJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyZtysme93Pl"
      },
      "source": [
        "def bag_of_words(prepared_data):\n",
        "\n",
        "  # DEIN CODE\n",
        "\n",
        "  return model,symbols,(min_word,max_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Vergleich der Modelle und Visualisierung\n",
        "\n",
        "Vergleiche die verschiedenen generativen n-Gram Modelle (insbesondere mit dem Bag-of-Words Modell). Was stellst du fest?\n",
        "Gibt es ein optimales n? Von was hängt dieses ab?\n",
        "Wo sind die Nachteile der n-Gram-Modelle und haben haben wir in der Vorlesung über Verbesserungen gesprochen?\n",
        "\n",
        "Erstelle den Graphen, der aus der Vorlesung bekannt ist.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbHhCQ8NFetH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfmEMZ7NiPhC"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}